---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---
  
<!-- ## About Me -->
I am joining [Hong Kong University of Science and Technology](https://hkust.edu.hk), [Department of Computer Science and Engineering](https://cse.hkust.edu.hk) as 
a tenure-track assistant professor in 2023 Fall. I obtained my PhD from [Carnegie Mellon University](https://www.cmu.edu), [Language Technologies Institute](https://www.lti.cs.cmu.edu) in the [School of Computer Science](https://www.cs.cmu.edu) in 2022, where I was co-advised by [Graham Neubig](http://www.phontron.com) and [Taylor Berg-Kirkpatrick](https://cseweb.ucsd.edu/~tberg/). Before that, I received the bachelor degree in Electronic Engineering from Shanghai Jiao Tong University in 2017. I also spent some time at Facebook AI Research (2019) and Salesforce Research (2020).  

<!-- I have interned at Facebook AI Research (2019), working with [Jiatao Gu](https://jiataogu.me) and [Marc'Aurelio Ranzato](https://ranzato.github.io); interned at Salesforce Research (2020), working with [Bryan McCann](https://bmccann.github.io); and visited Machine Learning Department of Carnegie Mellon University (2016), working with [Zhiting Hu](http://zhiting.ucsd.edu) and [Eric Xing](https://www.cs.cmu.edu/~epxing/).  -->

### <span style="color:red">Prospective Students:</span>
I am always actively looking for strong and self-motivated students to join us! I have multiple PhD/Mphil openings starting in 2024 Fall at HKUST, as well as RA positions starting anytime. If you are interested, please drop me an email with your CV (the email subject line should include "Prospective PhD/Mphil/RA Student"). For 2024 Fall PhD/Mphil applicants, I encourage you to reach out as soon as possible to ensure consideration. I may not be able to respond to all emails, sorry. 

## Research Interests
        
I am generally interested in natural language processing and machine learning. Current interests are on several aspects around large language models: 
- parameter-efficient tuning -- efficiency, modular, composition 
- multi-step reasoning
- factuality -- method and evaluation
- evaluation of foundation models -- knowledge, reasoning, tool use, etc.
- data-centric approaches 
- language model as agent


<!-- My research covers (latent-variable) generative models, controllable text generation, efficient text generation, and non-parametric language models.  -->


# Publications
Most recent publications on [Google Scholar](https://scholar.google.com/citations?user=BIFGeoUAAAAJ&hl=en&authuser=1).  
\* denotes co-first authors, $^\dagger$ denotes corresponding author/main advisor

**C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models**  
Yuzhen Huang\*, Yuzhuo Bai\*, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, *Junxian He*$^\dagger$  
Preprint 2023. [[arxiv]](https://arxiv.org/abs/2305.08322) [[github]](https://github.com/SJTU-LIT/ceval) [[website]](https://cevalbenchmark.com)

**Mega: Moving Average Equipped Gated Attention**  
Xuezhe Ma\*, Chunting Zhou\*, Xiang Kong, *Junxian He*, Liangke Gui, Graham Neubig, Jonathan May, Luke Zettlemoyer  
ICLR 2023. [[arxiv]](https://arxiv.org/abs/2209.10655)


# Service
Area Chair: EMNLP22, ACL23    
Reviewer: ICLR, NeurIPS, ICML, ACL, EMNLP, NAACL, ARR, TMLR

# Awards
Baidu PhD Fellowship, class of 2020 (10 recipients worldwide)  
Outstanding Undergraduate Thesis in SJTU (top 1%)  
National Scholarship in China (2014/2015/2016)
